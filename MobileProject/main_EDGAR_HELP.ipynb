{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Node 5fd22109-7e01-4065-9ca2-9a71dffe7c8e"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tdmclient import ClientAsync\n",
    "client = ClientAsync()\n",
    "node = await client.wait_for_node()\n",
    "\n",
    "await node.lock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Local_NAV as local\n",
    "from Global_NAV import *\n",
    "import MotionCTR2 as mctrl\n",
    "import Kal_FILTER as kfil\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import IPython\n",
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "import Kidnapping as kidnap\n",
    "\n",
    "import matplotlib \n",
    "from matplotlib import colors\n",
    "from cvision_2 import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Global variables\n",
    "\n",
    "state = 0\n",
    "obst = [0,0,0,0,0]\n",
    "motors_speed = [0,0]\n",
    "not_arrived = True\n",
    "k = 0\n",
    "C_conv_toThymio_right = 67.60908181\n",
    "C_conv_toThymio_left = 67.82946137\n",
    "L = 95 #mm\n",
    "R = 23 #mm\n",
    "Ts = 0.5 # sampling time\n",
    "# Initial state covariance matrix\n",
    "P_k_minus_1 = np.array([[0.1, 0, 0],\n",
    "                        [0, 0.1, 0],\n",
    "                        [0, 0, 0.1]])\n",
    "P_k = P_k_minus_1\n",
    "threshold_for_convergence = 20\n",
    "threshold_for_convergence_local = 100\n",
    "speed0 = 60\n",
    "speedGain = 30\n",
    "\n",
    "path = []\n",
    "while True:\n",
    "    if state == 0:\n",
    "        # Create an adjusted dictionary of 5 markers of 4x4 bits.\n",
    "        aruco_dictionary = cv2.aruco.extendDictionary(5, 4)\n",
    "        # Assign a marker to each objects to be recognized on the map\n",
    "        # Generate three reference markers\n",
    "        detector = create_detector(aruco_dictionary)\n",
    "       \n",
    "\n",
    "        for i in range(5):\n",
    "            marker = cv2.aruco.generateImageMarker(aruco_dictionary, i, 500, np.zeros((6, 6, 1), dtype=\"uint8\"), 1)\n",
    "            cv2.imwrite('marker' + str(i) +'.jpg', marker)\n",
    "        # Definition of the references used to map the space\n",
    "        ref1 = Obj(0, np.array([[0,0]]))\n",
    "        ref2 = Obj(1, np.array([[0,0]]))\n",
    "        ref3 = Obj(2, np.array([[0,0]]))\n",
    "\n",
    "       \n",
    "        # Definition of the objects to recognize in the space\n",
    "        goal = Obj(3, np.array([[]]))\n",
    "        thymio = Obj(4, np.array([[]]))\n",
    "\n",
    "        map = Map(ref1,ref2,ref3,920,660)\n",
    "\n",
    "        objects = np.array([ref1,ref2, ref3, goal, thymio])\n",
    "\n",
    "        # Open the video\n",
    "        # For a livestream\n",
    "        video = cv2.VideoCapture(0)\n",
    "        # or with a prerecorded video\n",
    "        #video = cv2.VideoCapture('test3.avi')\n",
    "        if (video.isOpened()== False): \n",
    "            print(\"Error opening video stream or file\")\n",
    "        \n",
    "        # Recognition of the view with the map and path planning\n",
    "        path_found = False\n",
    "        while not (path_found):\n",
    "            ret, frame = video.read()\n",
    "            if ret == True:\n",
    "                clear_output(wait=True)\n",
    "                show_frame(frame)\n",
    "\n",
    "                detect_objects(frame, detector, objects)\n",
    "                if map.references_detected():\n",
    "                    print('Goal position', map.localize(goal))\n",
    "                    matrix = map.matrix(frame, thymio, goal)\n",
    "\n",
    "                    if check_matrix(matrix):\n",
    "                        path, visitedNodes = global_path(matrix)\n",
    "                        print_path(matrix, path, visitedNodes)\n",
    "                        path = np.multiply(path, 50)\n",
    "                        N = np.size(path,1)\n",
    "                        path_found = True\n",
    "                        state_estimate_k = np.array([map.localize(thymio)[0][0],map.localize(thymio)[0][1],-vector_to_angle(map.vec_img2map(thymio.get_img_dir()[0]))])\n",
    "\n",
    "        # Navigation\n",
    "        state = 1    \n",
    "        # Global navigation state\n",
    "    if state == 1:\n",
    "        current_states= [[]]\n",
    "        path_to_follow = [[path[0,k],path[1,k]]]\n",
    "        while not_arrived == True:\n",
    "\n",
    "            #CHECK FOR OBSTACLES\n",
    "            prox_horizontal = local.read_prox_sensors(node,client)\n",
    "            obst = [prox_horizontal[0], prox_horizontal[1], prox_horizontal[2], prox_horizontal[3], prox_horizontal[4]]\n",
    "            state = local.update_state(state,obst,client)\n",
    "            if state == 2:\n",
    "                break\n",
    "\n",
    "            #CHECK KIDNAPPING\n",
    "            if kidnap.read_acc_sensors(node,client) > 21:\n",
    "                print(\"---------------KIDNAPPING DETECTED-----------------\")\n",
    "                mctrl.stop_motors(node)\n",
    "                time.sleep(1)\n",
    "                state = 0\n",
    "                break\n",
    "\n",
    "\n",
    "            # Path following: control law\n",
    "            print(\"Position final goal\", [path[0,k],path[1,k]])\n",
    "            vr, vl = mctrl.control_law(state_estimate_k, path[0,k], path[1,k],speed0,speedGain)\n",
    "            #control_vector_k_minus_1 = mctrl.convert_velocity2vw(vr,vl,C_conv_toThymio_right, C_conv_toThymio_left,L,R) if we use [v,w] as control variables in state space model\n",
    "            control_vector_k_minus_1 = np.array([vr/C_conv_toThymio_right,vl/C_conv_toThymio_left]) #if we use [vr,vl] as control variables in state space model\n",
    "            print(\"Control vector\", control_vector_k_minus_1)\n",
    "            mctrl.set_motors(vl,vr,node)\n",
    "            \n",
    "            \n",
    "            # Take Thymio position if camera is not obstructed and update state estimate with kalman filter\n",
    "            if video.isOpened:\n",
    "                ret, frame = video.read()\n",
    "                new_frame = frame.copy()\n",
    "                if ret == True:\n",
    "                    \n",
    "                    #clear_output(wait=True)\n",
    "                    detect_objects(frame, detector, objects)\n",
    "                    \n",
    "                    ref1.draw(new_frame, [255,0,0])\n",
    "                    ref2.draw(new_frame, [0,255,0])\n",
    "                    ref3.draw(new_frame, [0,0,255])\n",
    "                    goal.draw(new_frame, [255,255,255])\n",
    "                    thymio.draw(new_frame, [255,0,0])\n",
    "                    #show_frame(new_frame)\n",
    "                    \n",
    "                    \n",
    "                    if map.references_detected(): #camera correctly working\n",
    "                        camera_obstructed = 0\n",
    "\n",
    "                        if np.size(map.localize(thymio), 0) > 0: #camera working correctly\n",
    "                            #take robot position from camera\n",
    "                            z_k_observation_vector = np.array([map.localize(thymio)[0][0],map.localize(thymio)[0][1],-vector_to_angle(map.vec_img2map(thymio.get_img_dir()[0]))])  \n",
    "                            print(\"Robot position and angle (rad)\", z_k_observation_vector)  \n",
    "                            # update state estimate with kalman filter\n",
    "                            #state_estimate_k = z_k_observation_vector #if you don't use kalman filter\n",
    "                            state_estimate_k,P_k_minus_1 = kfil.ekf(z_k_observation_vector, state_estimate_k, control_vector_k_minus_1, P_k_minus_1, Ts, camera_obstructed)\n",
    "                            current_states.append(state_estimate_k)\n",
    "                            print(\"State_estimate_k\", state_estimate_k)\n",
    "                            print(\"temporary goal: \", [path[0,k], path[1,k]])   \n",
    "                    else: #camera obstructed\n",
    "                        camera_obstructed = 1\n",
    "                        print(\"WARNING:NOCAMERA--WARNING:NOCAMERA--WARNIN:NOCAMERA--WARNING:NOCAMERA\")\n",
    "                        #Measurements are not updated by camera but only by kalman filter\n",
    "                        state_estimate_k,P_k_minus_1 = kfil.ekf(z_k_observation_vector, state_estimate_k, control_vector_k_minus_1, P_k_minus_1, Ts, camera_obstructed) \n",
    "                        print(\"State_estimate_k\", state_estimate_k)\n",
    "                        print(\"temporary goal: \", [path[0,k], path[1,k]])\n",
    "\n",
    "            # Check if the robot has arrived to the temporary goal\n",
    "            if np.linalg.norm(state_estimate_k[:2] - np.array([path[0,k], path[1,k]])) < threshold_for_convergence:\n",
    "                if k >= N-1:\n",
    "                    not_arrived = False\n",
    "                    print('The robot has arrived to the final goal')\n",
    "                    mctrl.stop_motors(node)\n",
    "                    #break\n",
    "\n",
    "                elif k + N//8 < N-1:\n",
    "                    k = k + N//8 # go to the next temporary goal \n",
    "                else:\n",
    "                    k = N-1   # go to the final goal\n",
    "                path_to_follow.append([path[0,k],path[1,k]])\n",
    "                print(\"position {} of {} in path: \".format(k,N-1))\n",
    "                \n",
    "\n",
    " \n",
    "            time.sleep(Ts)   \n",
    "\n",
    "\n",
    "    # Local navigation state\n",
    "    if state == 2:         \n",
    "        #We get the sensor value\n",
    "        prox_horizontal = local.read_prox_sensors(node,client)\n",
    "        \n",
    "        #We fill the obst list with the sensor value\n",
    "        obst = [prox_horizontal[0], prox_horizontal[1], prox_horizontal[2], prox_horizontal[3], prox_horizontal[4]]\n",
    "        \n",
    "        #We calculate the motor speed to avoid the object -> potential field method\n",
    "        motors_speed = local.local_navigation(obst)\n",
    "        \n",
    "        #We set the motor speed\n",
    "        mctrl.set_motors(motors_speed[0],motors_speed[1],node)       \n",
    "        #We update the state\n",
    "        state = local.update_state(state,obst,client)\n",
    "        if state == 1:\n",
    "            vr = speed0\n",
    "            vl = speed0\n",
    "            mctrl.set_motors(vl,vr,node)\n",
    "            if k + 40 < N-1:\n",
    "                k = k+40\n",
    "            else: \n",
    "                k = N-10\n",
    "            print(\"koala\")\n",
    "             \n",
    "            \n",
    "\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate x and y coordinates\n",
    "x_path, y_path = zip(*path_to_follow)\n",
    "x_state, y_state = zip(*current_states)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_path, y_path, label='Path to Follow', marker='o')\n",
    "plt.plot(x_state, y_state, label='Current States', marker='x')\n",
    "plt.xlabel('X Coordinate')\n",
    "plt.ylabel('Y Coordinate')\n",
    "plt.title('Path to Follow vs Current States')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'Local_NAV' has no attribute 'global_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\elisa\\Desktop\\MobileProject\\main_EDGAR_HELP.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/elisa/Desktop/MobileProject/main_EDGAR_HELP.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m motors_speed \u001b[39m=\u001b[39m local\u001b[39m.\u001b[39mglobal_path()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/elisa/Desktop/MobileProject/main_EDGAR_HELP.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m local\u001b[39m.\u001b[39mset_motors(motors_speed[\u001b[39m0\u001b[39m],motors_speed[\u001b[39m1\u001b[39m],node) \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/elisa/Desktop/MobileProject/main_EDGAR_HELP.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m local\u001b[39m.\u001b[39mstop_motors(node)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'Local_NAV' has no attribute 'global_path'"
     ]
    }
   ],
   "source": [
    "motors_speed = local.global_path()\n",
    "local.set_motors(motors_speed[0],motors_speed[1],node) \n",
    "local.stop_motors(node) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mctrl.stop_motors(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
