{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Node 5fd22109-7e01-4065-9ca2-9a71dffe7c8e"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tdmclient import ClientAsync\n",
    "client = ClientAsync()\n",
    "node = await client.wait_for_node()\n",
    "await node.lock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Local_NAV as local\n",
    "import Global_NAV as glob\n",
    "import Motion_CTR_ as mctrl\n",
    "import Kalman_FIL as kfil\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import IPython\n",
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "from matplotlib import colors\n",
    "from Computer_VIS import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robot position :  [-0.28922092 -5.46318036]\n",
      "Robot direction :  -1.7653837640900176\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.8.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\core\\src\\mean.dispatch.cpp:516: error: (-215:Assertion failed) !_src.empty() in function 'cv::meanStdDev'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-0e120e961513>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;31m# Global navigation state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m         \u001b[0mmatrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mobjects\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mthymio\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgoal\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrefs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m         \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[0mN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\BASICS_MOBILE_ROBOTICS\\Computer_VIS.py\u001b[0m in \u001b[0;36mmatrix\u001b[1;34m(self, frame, objects, robot, goal, refs)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[1;31m# Write the goal position in the matrix with a 3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 245\u001b[1;33m         \u001b[0mmap_pos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlocalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgoal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap_pos\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap_pos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\BASICS_MOBILE_ROBOTICS\\Computer_VIS.py\u001b[0m in \u001b[0;36mlocalize\u001b[1;34m(self, frame, obj)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mlocalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m         \u001b[0mimg_coordinates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_img_pos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m         \u001b[0mmap_pos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_coordinates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\BASICS_MOBILE_ROBOTICS\\Computer_VIS.py\u001b[0m in \u001b[0;36mget_img_pos\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_img_pos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimg_pos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\BASICS_MOBILE_ROBOTICS\\Computer_VIS.py\u001b[0m in \u001b[0;36mdetect\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdetect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[0mcontours\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_contours\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolor_obj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolor_tolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[0mcentroids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[0msides\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\BASICS_MOBILE_ROBOTICS\\Computer_VIS.py\u001b[0m in \u001b[0;36mfind_contours\u001b[1;34m(frame, color, c)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfind_contours\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m     \u001b[0mlower_b\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupper_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupper_lower_color\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;31m# Find contours in the binary mask\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\BASICS_MOBILE_ROBOTICS\\Computer_VIS.py\u001b[0m in \u001b[0;36mupper_lower_color\u001b[1;34m(color_sample, c)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mupper_lower_color\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolor_sample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;31m# Define the upper and lower limit of the color given the sample image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m     \u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstddev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmeanStdDev\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolor_sample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m     \u001b[0mlower_color\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmean\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mstddev\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[0mupper_color\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmean\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mstddev\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.8.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\core\\src\\mean.dispatch.cpp:516: error: (-215:Assertion failed) !_src.empty() in function 'cv::meanStdDev'\n"
     ]
    }
   ],
   "source": [
    "#Global variables\n",
    "state = 0\n",
    "obst = [0,0,0,0,0]\n",
    "motors_speed = [0,0]\n",
    "not_arrived = True\n",
    "k = 0\n",
    "C_conv_toThymio_right = 67.60908181\n",
    "C_conv_toThymio_left = 67.82946137\n",
    "L = 104\n",
    "R = 20\n",
    "Ts = 0.02 # sampling time\n",
    "\n",
    "\n",
    "if state == 0:\n",
    "    # Definition of the references used to map the space\n",
    "    color_ref1 = cv2.imread('ref1.jpg')\n",
    "    ref1 = Obj(color_ref1, 50, 2, np.array([[0,0]]))\n",
    "    color_ref2 = cv2.imread('ref2.jpg')\n",
    "    ref2 = Obj(color_ref2, 100, 2, np.array([[0,0]]))\n",
    "    color_ref3 = cv2.imread('ref3.jpg')\n",
    "    ref3 = Obj(color_ref3, 100, 2, np.array([[0,0]]))\n",
    "    map = Map(ref1, ref2, ref3)\n",
    "\n",
    "    # Definition of the objects to recognize in the space\n",
    "    color_obstacle = cv2.imread('obstacle.jpg')\n",
    "    obstacle = Obj(color_obstacle, 700, 4)\n",
    "    color_goal = cv2.imread('goal.jpg')\n",
    "    goal = Obj(color_goal, 100, 4)\n",
    "    color_left_mark = cv2.imread('left_mark.jpg')\n",
    "    left_mark = Obj(color_left_mark, 50, 2)\n",
    "    color_right_mark = cv2.imread('right_mark.jpg')\n",
    "    right_mark = Obj(color_right_mark, 50, 2)\n",
    "    thymio = Robot(right_mark, left_mark)\n",
    "\n",
    "    # Open the video\n",
    "    video = cv2.VideoCapture('test3.avi')\n",
    "    if (video.isOpened()== False): \n",
    "        print(\"Error opening video stream or file\")\n",
    "\n",
    "    # the objects to draw on the map in the matrix format\n",
    "    objects = [obstacle]\n",
    "    refs = [ref1, ref2, ref3]\n",
    "    detected = False\n",
    "    while not (detected):\n",
    "        ret, frame = video.read()\n",
    "        if ret == True:\n",
    "            if np.size(thymio.localize(frame,map)[1], 0) > 0:\n",
    "                detected = True\n",
    "                print('Robot position : ', thymio.localize(frame,map)[0])\n",
    "                print('Robot direction : ', vector_to_angle(thymio.localize(frame,map)[1]))\n",
    "    state_estimate_k = np.array([thymio.localize(frame,map)[0,0],thymio.localize(frame,map)[0,1],vector_to_angle(thymio.localize(frame,map)[1])]) \n",
    "    state = 1\n",
    "        \n",
    "    # Global navigation state\n",
    "    if state == 1:\n",
    "        matrix = map.matrix(frame,objects,thymio,goal,refs)\n",
    "        path = glob.global_path(matrix)\n",
    "        N = len(path)\n",
    "        \n",
    "        while not_arrived == True:\n",
    "            prox_horizontal = local.read_prox_sensors(node,client)\n",
    "            obst = [prox_horizontal[0], prox_horizontal[1], prox_horizontal[2], prox_horizontal[3], prox_horizontal[4]]\n",
    "            state = local.update_state(state,obst,client)\n",
    "            x_goal_temp = path[1,k]\n",
    "            y_goal_temp = path[2,k]\n",
    "            if k < N-1:\n",
    "                theta_goal_temp = (math.atan2(x_goal[k+1]-x_goal[k], y_goal[k+1]-y_goal[k]) + np.pi) % (np.pi/2) - np.pi\n",
    "            v, w = mctrl.control_law(state_estimate_k, x_goal_temp, y_goal_temp, theta_goal_temp, Kv, Kp_alpha, Kp_beta)\n",
    "            vr, vl = mctrl.convert_velocity2RL(v,w,C_conv_toThymio_right, C_conv_toThymio_left,L,R)\n",
    "            vr = int(vr)\n",
    "            vl = int(vl)\n",
    "            set_motors(vl,vr,node)\n",
    "            \n",
    "            if camera_obstructed == 1 or video.isOpened == False:\n",
    "                z_k_observation_vector = np.array([state_estimate_k[0], state_estimate_k[1], state_estimate_k[2]])\n",
    "            if camera_obstructed == 0 :\n",
    "                # Capture frame-by-frame\n",
    "                ret, frame = video.read()\n",
    "                if ret == True:\n",
    "                    if np.size(thymio.localize(frame,map)[1], 0) > 0:\n",
    "                        print('Robot position : ', thymio.localize(frame,map)[0])\n",
    "                        print('Robot direction : ', vector_to_angle(thymio.localize(frame,map)[1]))\n",
    "                        #time.sleep(0.4)\n",
    "                        clear_output(wait=True)\n",
    "                        z_k_observation_vector = np.array([thymio.localize(frame,map)[0,0],thymio.localize(frame,map)[0,1],vector_to_angle(thymio.localize(frame,map)[1])]) \n",
    "                        [state_estimate_k,_] = kfil.ekf(z_k_observation_vector, state_estimate_k, [v,w] , P_k_minus_1, Ts)\n",
    "            if np.linalg.norm(state_estimate_k[:2] - np.array([x_goal_temp, y_goal_temp])) < threshold_for_convergence:\n",
    "                k = k+1\n",
    "                if k == N-1:\n",
    "                    not_arrived = False\n",
    "                    print('The robot has arrived to the final goal')\n",
    "            break \n",
    "            time.sleep(Ts) \n",
    "        state = 3                                               \n",
    "\n",
    "\n",
    "    # Local navigation state\n",
    "    if state == 2: \n",
    "        \n",
    "        #We get the sensor value\n",
    "        prox_horizontal = local.read_prox_sensors(node,client)\n",
    "        \n",
    "        #We fill the obst list with the sensor value\n",
    "        obst = [prox_horizontal[0], prox_horizontal[1], prox_horizontal[2], prox_horizontal[3], prox_horizontal[4]]\n",
    "        \n",
    "        #We calculate the motor speed to avoid the object -> potential field method\n",
    "        motors_speed = local.local_navigation(obst)\n",
    "        \n",
    "        #We set the motor speed\n",
    "        mctrl.set_motors(motors_speed[0],motors_speed[1],node)       \n",
    "        \n",
    "        #We update the state\n",
    "        state = local.update_state(state,obst,client)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "motors_speed = local.global_path()\n",
    "local.set_motors(motors_speed[0],motors_speed[1],node) \n",
    "local.stop_motors(node) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
